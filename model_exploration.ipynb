{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# EXPLORING PERFORMANCES OF GENERATED MODELS\n",
    "# We run a bunch of games to see the \n",
    "##############################################\n",
    "\n",
    "\n",
    "# INITIALIZE ALL RELEVANT VARIBLES AND FUNCTIONS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bipedal_walker_wrapper import BipedalWalkerAgent\n",
    "\n",
    "ID_DIFF = 10000 # The algorithm saves a new model each 10000 training steps\n",
    "NUMBER_OF_GAMES = 10 # For each model we play 10 games\n",
    "NUMBER_OF_LEVELS = 3 # We have 3 difficulty levels\n",
    "\n",
    "save_plots = True\n",
    "agent = BipedalWalkerAgent()\n",
    "\n",
    "def set_difficulty_level(level):\n",
    "    if level == 0:\n",
    "        # Easy level\n",
    "        agent.set_environment_type(hardcore=False, super_easy=True)\n",
    "    elif level == 1:\n",
    "        # Medium level\n",
    "        agent.set_environment_type(hardcore=False, super_easy=False)\n",
    "    elif level == 2:\n",
    "        # Hard level\n",
    "        agent.set_environment_type(hardcore=True, super_easy=False)\n",
    "    else:\n",
    "        print('Unknown difficulty level')\n",
    "\n",
    "def evaluate_batch(difficulty_level, difficulty_train, model_ids):\n",
    "    # Evaluates the models specified in the model_ids list trained on difficulty_train level\n",
    "    # against the difficulty level specifies\n",
    "    n_wins = 0\n",
    "    n_draws = 0\n",
    "    n_loss = 0\n",
    "    \n",
    "    set_difficulty_level(difficulty_level)\n",
    "    agent.set_model_difficulty(difficulty_train)\n",
    "    average_rewards = []\n",
    "    win_percentage = []\n",
    "    draw_percentage = []\n",
    "    loss_percentage = []\n",
    "    \n",
    "    # Iterate over each model in the batch\n",
    "    for model in model_ids:\n",
    "        # Load the model\n",
    "        agent.load_model(model)\n",
    "        total_model_reward = 0.0\n",
    "        n_wins = 0\n",
    "        n_draws = 0\n",
    "        n_losses = 0\n",
    "\n",
    "        # play NUMBER_OF_GAMES time to average the results\n",
    "        for i in range(NUMBER_OF_GAMES):\n",
    "            outcome, reward = agent.play()\n",
    "            total_model_reward += reward\n",
    "\n",
    "            if outcome == 1:\n",
    "                n_wins += 1\n",
    "            elif outcome == 0:\n",
    "                n_draws += 1\n",
    "            else:\n",
    "                n_losses +=1\n",
    "                \n",
    "        average_rewards.append(total_model_reward / NUMBER_OF_GAMES)\n",
    "        win_percentage.append(n_wins / NUMBER_OF_GAMES)\n",
    "        draw_percentage.append(n_draws / NUMBER_OF_GAMES)\n",
    "        loss_percentage.append(n_losses / NUMBER_OF_GAMES)\n",
    "    \n",
    "    outcomes = (win_percentage, draw_percentage, loss_percentage)\n",
    "    return outcomes, average_rewards\n",
    "\n",
    "def plot_(title, data, labels = [], lim = None, filename='file'):\n",
    "    %matplotlib inline\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 5) # (w, h)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.title(title)\n",
    "    a1 = ax.plot(data[0])\n",
    "    a2 = ax.plot(data[1])\n",
    "    a3 = ax.plot(data[2])\n",
    "\n",
    "    plt.legend((a1[0], a2[0], a3[0]), ('Easy', 'Medium', 'Hard'))\n",
    "    \n",
    "    if len(labels) > 0:\n",
    "        x = range(len(labels))\n",
    "        plt.xticks(x, labels, rotation='vertical')\n",
    "    \n",
    "    if lim != None:\n",
    "        plt.ylim(lim[0], lim[1])\n",
    "    plt.show()\n",
    "    \n",
    "    if save_plots:\n",
    "        path = 'img/model_exploration/' + filename + '.png'\n",
    "        fig.savefig(path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "# PLAYING AGAINS LEVEL 0\n",
      "#########################\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File ./models/easy/model-0.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fcc8896e2010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMBER_OF_LEVELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#########################\\n# PLAYING AGAINS LEVEL {}\\n#########################'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutcomes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_difficulty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ids_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moutcomes_model_easy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutcomes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mrewards_model_easy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-798a832d6dab>\u001b[0m in \u001b[0;36mevaluate_batch\u001b[0;34m(difficulty_level, difficulty_train, model_ids)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtotal_model_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mn_wins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/trueskill_validation/bipedal_walker_wrapper.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, id_number)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_meta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# saver = tf.train.import_meta_graph('/home/stefano/Projects/gym_bipedal_walker_v2_solution/experiments/logs/bipedal_walker_easy/model-200000.ckpt.meta')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_environments/env_gym/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1674\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_environments/env_gym/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1685\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_environments/env_gym/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File ./models/easy/model-0.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# EXPLORE MODELS TRAINED ON THE EASY ENVIRONMENT\n",
    "##################################################\n",
    "\n",
    "model_difficulty = 'easy'\n",
    "number_of_models = 38\n",
    "starting_model = 0\n",
    "ending_model = starting_model + ID_DIFF * number_of_models\n",
    "model_ids_batch = np.linspace(starting_model, ending_model, number_of_models+1, dtype=int)\n",
    "easy_ids_batch = model_ids_batch\n",
    "\n",
    "outcomes_model_easy = {}\n",
    "rewards_model_easy = []\n",
    "\n",
    "# For each difficulty level\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    print('#########################\\n# PLAYING AGAINS LEVEL {}\\n#########################'.format(i))\n",
    "    outcomes, rewards = evaluate_batch(i, model_difficulty, model_ids_batch)\n",
    "    outcomes_model_easy[i] = outcomes\n",
    "    rewards_model_easy.append(rewards)\n",
    "\n",
    "    \n",
    "%store easy_ids_batch\n",
    "%store rewards_model_easy\n",
    "%store outcomes_model_easy\n",
    "print('########### COMPUTATION COMPLETED ###########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# PLOT THE RESULTS\n",
    "##############################\n",
    "\n",
    "%store -r easy_ids_batch\n",
    "%store -r rewards_model_easy\n",
    "%store -r outcomes_model_easy\n",
    "\n",
    "win_perc_easy = []\n",
    "draw_perc_easy = []\n",
    "loss_perc_easy = []\n",
    "\n",
    "\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    win_perc_easy.append(outcomes_model_easy[i][0])\n",
    "    draw_perc_easy.append(outcomes_model_easy[i][1])\n",
    "    loss_perc_easy.append(outcomes_model_easy[i][2])\n",
    "    \n",
    "%store win_perc_easy\n",
    "%store draw_perc_easy\n",
    "%store loss_perc_easy\n",
    "\n",
    "# We expect these models to be able to solve only easy environments\n",
    "\n",
    "plot_('Average rewards for easy models',rewards_model_easy, labels=easy_ids_batch, filename='easy/rewards')\n",
    "plot_('Win percentage for easy models', win_perc_easy, labels=easy_ids_batch, lim=(-0.05,1.05), filename='easy/win')\n",
    "plot_('Draw percentage for easy models', draw_perc_easy, labels=easy_ids_batch, lim=(-0.05,1.05), filename='easy/draw')\n",
    "plot_('Loss percentage for easy models', loss_perc_easy, labels=easy_ids_batch, lim=(-0.05,1.05), filename='easy/loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# EVALUATE MODELS TRAINED IN MEDIUM ENVIRONMENT\n",
    "################################################\n",
    "\n",
    "model_difficulty = 'medium'\n",
    "number_of_models = 15\n",
    "starting_model = 150000\n",
    "ending_model = starting_model + ID_DIFF * number_of_models\n",
    "model_ids_batch = np.linspace(starting_model, ending_model, number_of_models+1, dtype=int)\n",
    "medium_ids_batch = model_ids_batch\n",
    "%store medium_ids_batch\n",
    "\n",
    "outcomes_model_medium = []\n",
    "rewards_model_medium = []\n",
    "\n",
    "# For each difficulty level\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    print('#########################\\n# PLAYING AGAINS LEVEL {}\\n#########################'.format(i))\n",
    "    outcomes, rewards = evaluate_batch(i, model_difficulty, model_ids_batch)\n",
    "    outcomes_model_medium.append(outcomes)\n",
    "    rewards_model_medium.append(rewards)\n",
    "\n",
    "%store rewards_model_medium\n",
    "%store outcomes_model_medium\n",
    "print('########### COMPUTATION COMPLETED ###########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# PLOT THE RESULTS\n",
    "#####################\n",
    "\n",
    "%store -r medium_ids_batch\n",
    "%store -r rewards_model_medium\n",
    "%store -r outcomes_model_medium\n",
    "\n",
    "win_perc_medium = []\n",
    "draw_perc_medium = []\n",
    "loss_perc_medium = []\n",
    "\n",
    "\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    win_perc_medium.append(outcomes_model_medium[i][0])\n",
    "    draw_perc_medium.append(outcomes_model_medium[i][1])\n",
    "    loss_perc_medium.append(outcomes_model_medium[i][2])\n",
    "    \n",
    "%store win_perc_medium\n",
    "%store draw_perc_medium\n",
    "%store loss_perc_medium\n",
    "\n",
    "\n",
    "# We expect these models to be able to solve easy and medium environments\n",
    "\n",
    "plot_('Average rewards for medium models',rewards_model_medium, labels=medium_ids_batch, filename='medium/rewards')\n",
    "plot_('Win percentage for medium models', win_perc_medium, labels=medium_ids_batch, lim=(-0.2,1.2), filename='medium/win')\n",
    "plot_('Draw percentage for medium models', draw_perc_medium, labels=medium_ids_batch, lim=(-0.2,1.2), filename='medium/draw')\n",
    "plot_('Loss percentage for medium models', loss_perc_medium, labels=medium_ids_batch, lim=(-0.2,1.2), filename='medium/loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# EVALUATE MODELS TRAINED IN HARD ENVIRONMENT\n",
    "################################################\n",
    "model_difficulty = 'hard'\n",
    "#model_ids_batch = [1530000, 1380000, 1490000, 1340000, 1460000, 1310000, 1060000, 1550000, 890000, 1400000]\n",
    "number_of_models = 60\n",
    "starting_model = 1000000\n",
    "ending_model = starting_model + ID_DIFF * number_of_models\n",
    "model_ids_batch = np.linspace(starting_model, ending_model, number_of_models+1, dtype=int)\n",
    "\n",
    "hard_ids_batch = model_ids_batch\n",
    "%store hard_ids_batch\n",
    "\n",
    "outcomes_model_hard = []\n",
    "rewards_model_hard = []\n",
    "\n",
    "# For each difficulty level\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    print('#########################\\n# PLAYING AGAINS LEVEL {}\\n#########################'.format(i))\n",
    "    outcomes, rewards = evaluate_batch(i, model_difficulty, model_ids_batch)\n",
    "    outcomes_model_hard.append(outcomes)\n",
    "    rewards_model_hard.append(rewards)\n",
    "\n",
    "%store rewards_model_hard\n",
    "%store outcomes_model_hard\n",
    "print('########### COMPUTATION COMPLETED ###########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# PLOT THE RESULTS\n",
    "#####################\n",
    "\n",
    "%store -r hard_ids_batch\n",
    "%store -r rewards_model_hard\n",
    "%store -r outcomes_model_hard\n",
    "\n",
    "win_perc_hard = []\n",
    "draw_perc_hard = []\n",
    "loss_perc_hard = []\n",
    "\n",
    "\n",
    "for i in range(NUMBER_OF_LEVELS):\n",
    "    win_perc_hard.append(outcomes_model_hard[i][0])\n",
    "    draw_perc_hard.append(outcomes_model_hard[i][1])\n",
    "    loss_perc_hard.append(outcomes_model_hard[i][2])\n",
    "    \n",
    "%store win_perc_hard\n",
    "%store draw_perc_hard\n",
    "%store loss_perc_hard\n",
    "\n",
    "\n",
    "# We expect these models to be able to solve easy and medium environments\n",
    "\n",
    "plot_('Average rewards for hard models',rewards_model_hard, labels=hard_ids_batch, filename='hard/rewards')\n",
    "plot_('Win percentage for hard models', win_perc_hard, labels=hard_ids_batch, lim=(-0.2,1.2), filename='hard/win')\n",
    "plot_('Draw percentage for hard models', draw_perc_hard, labels=hard_ids_batch, lim=(-0.2,1.2), filename='hard/draw')\n",
    "plot_('Loss percentage for hard models', loss_perc_hard, labels=hard_ids_batch, lim=(-0.2,1.2), filename='hard/loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFun(e):\n",
    "    return e[1]\n",
    "\n",
    "%store -r hard_ids_batch\n",
    "%store -r win_perc_hard\n",
    "zipped_win = []\n",
    "zipped_draw = []\n",
    "zipped_loss = []\n",
    "for i in range(len(hard_ids_batch)):\n",
    "    zipped_win.append([hard_ids_batch[i], win_perc_hard[2][i]])\n",
    "    zipped_draw.append([hard_ids_batch[i], draw_perc_hard[2][i]])\n",
    "    zipped_loss.append([hard_ids_batch[i], loss_perc_hard[2][i]])\n",
    "    \n",
    "zipped_loss.sort(reverse=True, key=myFun)\n",
    "print(zipped_loss[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
